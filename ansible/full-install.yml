---
- name: "ğŸš€ Complete Hadoop Cluster Automated Installation"
  hosts: localhost
  connection: local
  gather_facts: yes
  
  vars:
    project_root: "{{ playbook_dir }}/.."
    required_services:
      - namenode
      - datanode1
      - datanode2
      - dashboard
      - spark-master
      - kafka
    
  tasks:
    # =============== PRE-INSTALLATION CHECKS ===============
    - name: "ğŸ“‹ Pre-installation checks"
      block:
        - name: "ğŸ³ Verify Docker is running"
          shell: docker info
          register: docker_check
          failed_when: docker_check.rc != 0
          
        - name: "ğŸ“ Verify docker-compose.yml exists"
          stat:
            path: "{{ project_root }}/docker-compose.yml"
          register: compose_file
          failed_when: not compose_file.stat.exists
          
        - name: "ğŸ” Check available disk space"
          shell: df -h . | tail -1 | awk '{print $4}'
          register: disk_space
          
        - name: "ğŸ’¾ Verify sufficient disk space (minimum 5GB)"
          fail:
            msg: "Insufficient disk space: {{ disk_space.stdout }}"
          when: disk_space.stdout | regex_replace('[^0-9.]', '') | float < 5.0
          ignore_errors: yes
          
      tags: [precheck]
    
    # =============== CLEANUP & PREPARATION ===============
    - name: "ğŸ§¹ Clean previous installation"
      block:
        - name: "â¹ï¸ Stop existing containers"
          shell: |
            cd "{{ project_root }}"
            docker-compose down --remove-orphans
          ignore_errors: yes
          
        - name: "ğŸ—‘ï¸ Remove old volumes (if fresh install)"
          shell: |
            cd "{{ project_root }}"
            docker-compose down -v
          when: fresh_install | default(false)
          ignore_errors: yes
          
        - name: "ğŸ³ Prune unused Docker resources"
          shell: docker system prune -f
          when: fresh_install | default(false)
          ignore_errors: yes
          
      tags: [cleanup]
    
    # =============== INFRASTRUCTURE DEPLOYMENT ===============
    - name: "ğŸ—ï¸ Deploy Hadoop Infrastructure"
      block:
        - name: "ğŸ“¦ Build and start all services"
          shell: |
            cd "{{ project_root }}"
            docker-compose up -d --build
          register: compose_result
          
        - name: "ğŸ“Š Verify containers are starting"
          shell: docker ps --format "{{.Names}}: {{.Status}}"
          register: initial_containers
          
        - name: "ğŸ“ Log initial container status"
          debug:
            msg: |
              Initial containers status:
              {{ initial_containers.stdout }}
              
      tags: [deploy]
    
    # =============== SERVICE READINESS CHECKS ===============
    - name: "â³ Wait for services to be ready"
      block:
        - name: "ğŸ• Wait for NameNode (critical service)"
          uri:
            url: "http://localhost:9870"
            method: GET
            status_code: 200
          register: namenode_ready
          until: namenode_ready.status == 200
          retries: 60  # 10 minutes
          delay: 10
          
        - name: "ğŸ• Wait for DataNodes to connect"
          shell: |
            timeout 300 bash -c '
            while true; do
              if docker exec namenode hdfs dfsadmin -report | grep -q "Live datanodes.*: 2"; then
                echo "DataNodes connected"
                exit 0
              fi
              echo "Waiting for DataNodes..."
              sleep 10
            done'
          register: datanodes_ready
          
        - name: "ğŸ• Wait for Dashboard"
          uri:
            url: "http://localhost:8501"
            method: GET
            status_code: 200
          register: dashboard_ready
          until: dashboard_ready.status == 200
          retries: 30
          delay: 10
          ignore_errors: yes
          
        - name: "ğŸ• Wait for Spark Master"
          uri:
            url: "http://localhost:8080"
            method: GET
            status_code: 200
          register: spark_ready
          until: spark_ready.status == 200
          retries: 20
          delay: 10
          ignore_errors: yes
          
        - name: "ğŸ• Wait for Hive to be ready"
          shell: |
            timeout 180 bash -c '
            while true; do
              if docker exec hive-server beeline -u jdbc:hive2://localhost:10000 -e "USE analytics; SHOW TABLES;" | grep -q "reviews"; then
                echo "Hive tables ready"
                exit 0
              fi
              echo "Waiting for Hive tables..."
              sleep 15
            done'
          register: hive_ready
          ignore_errors: yes
          
      tags: [readiness]
    
    # =============== HDFS INITIALIZATION ===============
    - name: "ğŸ“ Initialize HDFS structure"
      block:
        - name: "ğŸ—‚ï¸ Create HDFS directories"
          shell: |
            docker exec namenode hdfs dfs -mkdir -p /data/text/existing
            docker exec namenode hdfs dfs -mkdir -p /data/text/scraped  
            docker exec namenode hdfs dfs -mkdir -p /data/images/existing
            docker exec namenode hdfs dfs -mkdir -p /data/images/scraped
            docker exec namenode hdfs dfs -mkdir -p /data/streaming
            docker exec namenode hdfs dfs -mkdir -p /data/processed
            docker exec namenode hdfs dfs -mkdir -p /data/ia_results
          register: hdfs_dirs
          
        - name: "ğŸ§ª Test HDFS write operations"
          shell: |
            echo "Ansible installation test $(date)" > /tmp/ansible_install_test.txt
            docker cp /tmp/ansible_install_test.txt namenode:/tmp/
            docker exec namenode hdfs dfs -put /tmp/ansible_install_test.txt /data/processed/
            docker exec namenode hdfs dfs -cat /data/processed/ansible_install_test.txt
          register: hdfs_test
          
      tags: [hdfs_init]
    
    # =============== DATA LOADING (if data-loader exists) ===============
    - name: "ğŸ“¥ Load initial data"
      block:
        - name: "ğŸ” Check if data-loader service exists"
          shell: docker-compose config --services | grep data-loader
          register: data_loader_exists
          ignore_errors: yes
          
        - name: "ğŸ“¥ Run data loader"
          shell: |
            cd "{{ project_root }}"
            docker-compose up data-loader
          when: data_loader_exists.rc == 0
          ignore_errors: yes
          
      tags: [data_loading]
    
    # =============== COMPREHENSIVE VALIDATION ===============
    - name: "âœ… Validate complete installation"
      block:
        - name: "ğŸ¥ Final health check all services"
          uri:
            url: "http://localhost:{{ item }}"
            method: GET
            status_code: 200
          loop:
            - 9870  # NameNode
            - 9864  # DataNode1
            - 9865  # DataNode2
            - 8501  # Dashboard
            - 8080  # Spark
          ignore_errors: yes
          register: final_health_check
          
        - name: "ğŸ§ª Test Hive connectivity"
          shell: |
            docker exec hive-server beeline -u jdbc:hive2://localhost:10000 -e "
            USE analytics;
            SELECT COUNT(*) as total_reviews FROM reviews;
            SELECT COUNT(*) as total_images FROM images;
            "
          register: hive_test
          ignore_errors: yes
          
        - name: "ğŸ“Š Generate installation report"
          shell: |
            echo "=== HADOOP CLUSTER INSTALLATION REPORT ===" > /tmp/install_report.txt
            echo "Installation completed: $(date)" >> /tmp/install_report.txt
            echo "" >> /tmp/install_report.txt
            echo "=== Container Status ===" >> /tmp/install_report.txt
            docker ps --format "{{.Names}}: {{.Status}}" >> /tmp/install_report.txt
            echo "" >> /tmp/install_report.txt
            echo "=== HDFS Status ===" >> /tmp/install_report.txt
            docker exec namenode hdfs dfsadmin -report | head -10 >> /tmp/install_report.txt
            echo "" >> /tmp/install_report.txt
            echo "=== HDFS Directory Structure ===" >> /tmp/install_report.txt
            docker exec namenode hdfs dfs -ls -R /data >> /tmp/install_report.txt
            echo "" >> /tmp/install_report.txt
            echo "=== Hive Test Results ===" >> /tmp/install_report.txt
            echo "{{ hive_test.stdout }}" >> /tmp/install_report.txt
            cat /tmp/install_report.txt
          register: install_report
          
      tags: [validation]
    
    # =============== SUCCESS SUMMARY ===============
    - name: "ğŸ‰ Installation Complete"
      debug:
        msg: |
          
          âœ… HADOOP CLUSTER SUCCESSFULLY INSTALLED!
          ==========================================
          
          ğŸš€ Automated by Ansible on: {{ ansible_date_time.iso8601 }}
          
          ğŸ“Š Services Deployed:
          {% for service in required_services %}
          â€¢ {{ service }}
          {% endfor %}
          â€¢ hive-metastore
          â€¢ hive-server
          
          ğŸ”— Access Points:
          â€¢ NameNode Web UI: http://localhost:9870
          â€¢ DataNode1: http://localhost:9864
          â€¢ DataNode2: http://localhost:9865
          â€¢ Streamlit Dashboard: http://localhost:8501
          â€¢ Spark Master: http://localhost:8080
          â€¢ Hive Server: jdbc:hive2://localhost:10000
          
          ğŸ“ HDFS Structure:
          /data/text/existing    - Pre-existing text data
          /data/text/scraped     - Scraped text data
          /data/images/existing  - Pre-existing images
          /data/images/scraped   - Scraped images
          /data/streaming        - Real-time data
          /data/processed        - Processed data
          /data/ia_results       - AI analysis results
          
          ğŸ“Š Hive Database:
          â€¢ Database: analytics
          â€¢ Tables: reviews, images, ia_results
          â€¢ Status: {{ 'Ready' if hive_ready.rc == 0 else 'Check logs' }}
          
          ğŸ¯ Next Steps:
          1. Visit http://localhost:9870 to browse HDFS
          2. Visit http://localhost:8501 for the dashboard
          3. Test Hive: docker exec -it hive-server beeline -u jdbc:hive2://localhost:10000
          4. Run: USE analytics; SHOW TABLES; SELECT * FROM reviews LIMIT 5;
          
          âœ… Installation automated successfully with Ansible!
      
      tags: [summary]