# ğŸš€ Hadoop Big Data Infrastructure

**Complete Hadoop cluster with real-time web scraping, data processing, and AI integration**

This repository contains the Hadoop infrastructure component of our **Hadoop & AI Project**, designed to handle big data processing, real-time web scraping, and seamless integration with AI services.

## âš ï¸ Important Prerequisites for Evaluators

**Before starting, please ensure the following setup:**

### 1. Install Git Bash (Required)
This project uses shell scripts that require Git Bash on Windows:

```bash
# Download and install Git for Windows from:
# https://git-scm.com/download/win

# After installation, always use Git Bash terminal for this project
```

### 2. Set File Permissions (Critical)
After cloning, you **must** give execution rights to script files:

```bash
# Navigate to project directory in Git Bash
cd adam_hadoop

# Grant execution permissions to all scripts
chmod +x scripts/*.sh
chmod +x *.sh
find . -name "*.sh" -exec chmod +x {} \;

# Verify permissions
ls -la scripts/
```

### 3. Docker Desktop Setup
```bash
# Ensure Docker Desktop is running with WSL2 backend
# Minimum requirements:
# - 8GB RAM allocated to Docker
# - 20GB free disk space
# - WSL2 enabled on Windows
```

**âš¡ Without these prerequisites, the deployment will fail!**

---

## ğŸ“‹ Overview

This project implements a production-ready Hadoop ecosystem that includes:

- **Multi-node Hadoop cluster** (1 NameNode + 2 DataNodes)
- **Real-time web scraping** with Kafka streaming
- **Spark processing** for data analytics
- **Hive data warehouse** for structured queries
- **Streamlit dashboard** for real-time monitoring
- **AI API integration** for machine learning workflows

## ğŸ—ï¸ Architecture

### High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Sources   â”‚â”€â”€â”€â–¶â”‚   Web Scraper   â”‚â”€â”€â”€â–¶â”‚      Kafka      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚â—€â”€â”€â”€â”‚  Spark Cluster  â”‚â—€â”€â”€â”€â”‚   Streaming     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Hive/Analyticsâ”‚â—€â”€â”€â”€â”‚  HDFS Storage   â”‚â—€â”€â”€â”€â”‚   Data Loading   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   AI API        â”‚
                    â”‚  Integration    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Technical Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PRESENTATION LAYER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Streamlit      â”‚  Hadoop Web UI  â”‚  Spark Web UI   â”‚  Kafka Manager          â”‚
â”‚  Dashboard      â”‚  (Port 9870)    â”‚  (Port 8080)    â”‚  (Port 9000)            â”‚
â”‚  (Port 8501)    â”‚                 â”‚                 â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              APPLICATION LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Web Scraper   â”‚  Spark Jobs     â”‚  Hive Queries   â”‚  AI Integration         â”‚
â”‚   - Reddit API  â”‚  - Analytics    â”‚  - Data Warehouseâ”‚  - REST API Client     â”‚
â”‚   - News RSS    â”‚  - ML Pipeline  â”‚  - SQL Interfaceâ”‚  - Batch Processing     â”‚
â”‚   - Image APIs  â”‚  - Streaming    â”‚  - Reporting    â”‚  - Real-time Inference  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PROCESSING LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Apache Kafka   â”‚  Apache Spark   â”‚  Apache Hive    â”‚  Data Loaders           â”‚
â”‚  - Message      â”‚  - Master Node  â”‚  - Metastore    â”‚  - External Datasets    â”‚
â”‚    Streaming    â”‚  - Worker Nodes â”‚  - Query Engine â”‚  - CSV/JSON Parsers     â”‚
â”‚  - Topic Mgmt   â”‚  - Distributed  â”‚  - JDBC Server  â”‚  - Data Validation      â”‚
â”‚                 â”‚    Computing    â”‚                 â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               STORAGE LAYER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      HDFS       â”‚   PostgreSQL    â”‚  Docker Volumes â”‚  Backup Storage         â”‚
â”‚  - NameNode     â”‚  - Hive Meta    â”‚  - Container    â”‚  - Incremental Backups â”‚
â”‚  - DataNode 1   â”‚  - Metadata     â”‚    Persistence  â”‚  - Configuration Backup â”‚
â”‚  - DataNode 2   â”‚  - Schema Info  â”‚  - Log Storage  â”‚  - Data Export/Import   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INFRASTRUCTURE LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Docker Engine  â”‚  Networking     â”‚  Monitoring     â”‚  Security               â”‚
â”‚  - Containers   â”‚  - Internal     â”‚  - Health       â”‚  - Network Isolation    â”‚
â”‚  - Orchestrationâ”‚    Network      â”‚    Checks       â”‚  - Access Control       â”‚
â”‚  - Resource     â”‚  - Port         â”‚  - Metrics      â”‚  - Data Encryption      â”‚
â”‚    Management   â”‚    Mapping      â”‚    Collection   â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture Details

#### 1. Data Ingestion Layer
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA INGESTION PIPELINE                     â”‚
â”‚                                                                 â”‚
â”‚  Web Sources                    Scraper Services               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Reddit API  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Reddit Scraper  â”‚             â”‚
â”‚  â”‚ News RSS    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ News Scraper    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ HackerNews  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ HN Scraper      â”‚         â”‚   â”‚
â”‚  â”‚ Image APIs  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Image Scraper   â”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚                                                             â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”¤
â”‚  â”‚                    KAFKA MESSAGE BROKER                     â”‚
â”‚  â”‚                                                              â”‚
â”‚  â”‚  Topics:                    Partitions:                     â”‚
â”‚  â”‚  â€¢ scraped-text     â”€â”€â”€â”€â–¶   â€¢ Partition 0: Reddit          â”‚
â”‚  â”‚  â€¢ scraped-images   â”€â”€â”€â”€â–¶   â€¢ Partition 1: News            â”‚
â”‚  â”‚  â€¢ scraped-metadata â”€â”€â”€â”€â–¶   â€¢ Partition 2: Images          â”‚
â”‚  â”‚                                                              â”‚
â”‚  â”‚  Features:                                                   â”‚
â”‚  â”‚  â€¢ Message persistence (7 days retention)                   â”‚
â”‚  â”‚  â€¢ Automatic scaling and load balancing                     â”‚
â”‚  â”‚  â€¢ Consumer group management                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Processing and Analytics Layer
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DISTRIBUTED PROCESSING LAYER                  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    APACHE SPARK CLUSTER                    â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  Master Node                 Worker Nodes                  â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚â”‚
â”‚  â”‚  â”‚ Resource    â”‚             â”‚ Worker 1 â”‚ Worker 2 â”‚       â”‚â”‚
â”‚  â”‚  â”‚ Manager     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2GB RAM  â”‚ 2GB RAM  â”‚       â”‚â”‚
â”‚  â”‚  â”‚ Job         â”‚             â”‚ 2 Cores  â”‚ 2 Cores  â”‚       â”‚â”‚
â”‚  â”‚  â”‚ Scheduler   â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  Processing Types:                                          â”‚â”‚
â”‚  â”‚  â€¢ Batch Processing    â”€â”€â”€â”€ Historical data analysis       â”‚â”‚
â”‚  â”‚  â€¢ Stream Processing   â”€â”€â”€â”€ Real-time data processing      â”‚â”‚
â”‚  â”‚  â€¢ ML Pipelines       â”€â”€â”€â”€ Machine learning workflows     â”‚â”‚
â”‚  â”‚  â€¢ Data Transformation â”€â”€â”€â”€ ETL operations                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                      APACHE HIVE                           â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  Components:                                                â”‚â”‚
â”‚  â”‚  â€¢ Metastore      â”€â”€â”€â”€ Schema and metadata management      â”‚â”‚
â”‚  â”‚  â€¢ Query Engine   â”€â”€â”€â”€ SQL query processing               â”‚â”‚
â”‚  â”‚  â€¢ JDBC Server    â”€â”€â”€â”€ External tool connectivity         â”‚â”‚
â”‚  â”‚  â€¢ Storage Format â”€â”€â”€â”€ Parquet, ORC optimization          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. Storage Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HADOOP DISTRIBUTED FILE SYSTEM             â”‚
â”‚                                                                 â”‚
â”‚  NameNode (Master)              DataNodes (Workers)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Metadata        â”‚           â”‚ DataNode1  â”‚ DataNode2  â”‚     â”‚
â”‚  â”‚ â€¢ File System   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Block      â”‚ Block      â”‚     â”‚
â”‚  â”‚   Tree          â”‚           â”‚ Storage    â”‚ Storage    â”‚     â”‚
â”‚  â”‚ â€¢ Block         â”‚           â”‚ 20GB       â”‚ 20GB       â”‚     â”‚
â”‚  â”‚   Locations     â”‚           â”‚ Capacity   â”‚ Capacity   â”‚     â”‚
â”‚  â”‚ â€¢ Replication   â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”‚   Policies      â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                 â”‚
â”‚  Data Organization:                                             â”‚
â”‚  /data/                                                         â”‚
â”‚  â”œâ”€â”€ text/                                                      â”‚
â”‚  â”‚   â”œâ”€â”€ existing/     â”€â”€ Pre-loaded datasets                  â”‚
â”‚  â”‚   â”œâ”€â”€ scraped/      â”€â”€ Real-time scraped content            â”‚
â”‚  â”‚   â””â”€â”€ processed/    â”€â”€ Cleaned and analyzed data            â”‚
â”‚  â”œâ”€â”€ images/                                                    â”‚
â”‚  â”‚   â”œâ”€â”€ metadata/     â”€â”€ Image information and tags           â”‚
â”‚  â”‚   â””â”€â”€ thumbnails/   â”€â”€ Generated preview images             â”‚
â”‚  â”œâ”€â”€ streaming/                                                 â”‚
â”‚  â”‚   â”œâ”€â”€ raw/          â”€â”€ Unprocessed stream data              â”‚
â”‚  â”‚   â””â”€â”€ processed/    â”€â”€ Real-time analytics results          â”‚
â”‚  â””â”€â”€ analytics/                                                 â”‚
â”‚      â”œâ”€â”€ reports/      â”€â”€ Generated analytical reports         â”‚
â”‚      â””â”€â”€ models/       â”€â”€ ML model artifacts                   â”‚
â”‚                                                                 â”‚
â”‚  Features:                                                      â”‚
â”‚  â€¢ Automatic replication (factor: 2)                           â”‚
â”‚  â€¢ Block size optimization (256MB)                             â”‚
â”‚  â€¢ Fault tolerance and recovery                                â”‚
â”‚  â€¢ Load balancing across DataNodes                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4. Data Flow Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      REAL-TIME DATA FLOW                       â”‚
â”‚                                                                 â”‚
â”‚  1. Data Ingestion                                              â”‚
â”‚     Web Sources â”€â”€â–¶ Scrapers â”€â”€â–¶ Kafka Topics                  â”‚
â”‚                     (Every 5min)  (Message Queue)              â”‚
â”‚                                                                 â”‚
â”‚  2. Stream Processing                                           â”‚
â”‚     Kafka â”€â”€â–¶ Spark Streaming â”€â”€â–¶ Data Validation              â”‚
â”‚              (Real-time)          (Quality Checks)             â”‚
â”‚                                                                 â”‚
â”‚  3. Data Storage                                                â”‚
â”‚     Validated Data â”€â”€â–¶ HDFS â”€â”€â–¶ Hive Tables                    â”‚
â”‚                       (Persistent) (Structured)                â”‚
â”‚                                                                 â”‚
â”‚  4. Analytics & AI                                              â”‚
â”‚     HDFS Data â”€â”€â–¶ Spark Jobs â”€â”€â–¶ AI API â”€â”€â–¶ Results            â”‚
â”‚                  (Batch)         (ML Models)  (Insights)       â”‚
â”‚                                                                 â”‚
â”‚  5. Visualization                                               â”‚
â”‚     Results â”€â”€â–¶ Dashboard â”€â”€â–¶ User Interface                   â”‚
â”‚              (Streamlit)     (Real-time Updates)               â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚                    BATCH DATA FLOW                         â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚  External Data â”€â”€â–¶ Data Loaders â”€â”€â–¶ HDFS                   â”‚ â”‚
â”‚ â”‚  (CSV, JSON)      (Validation)      (Storage)              â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚  HDFS â”€â”€â–¶ Spark Batch Jobs â”€â”€â–¶ Hive â”€â”€â–¶ Reports            â”‚ â”‚
â”‚ â”‚         (Analytics)           (Warehouse) (Dashboard)       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5. Monitoring and Observability Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONITORING ARCHITECTURE                     â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                  METRICS COLLECTION                        â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  System Metrics          Application Metrics               â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚â”‚
â”‚  â”‚  â”‚ Docker      â”‚         â”‚ HDFS Stats      â”‚               â”‚â”‚
â”‚  â”‚  â”‚ Containers  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Spark Jobs      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚â”‚
â”‚  â”‚  â”‚ Resource    â”‚         â”‚ Kafka Topics    â”‚          â”‚    â”‚â”‚
â”‚  â”‚  â”‚ Usage       â”‚         â”‚ Scraper Status  â”‚          â”‚    â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚â”‚
â”‚  â”‚                                                       â”‚    â”‚â”‚
â”‚  â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚                          â”‚       STREAMLIT DASHBOARD       â”‚â”‚
â”‚  â”‚                          â”‚                                  â”‚â”‚
â”‚  â”‚                          â”‚ â€¢ Real-time cluster health      â”‚â”‚
â”‚  â”‚                          â”‚ â€¢ Data processing statistics    â”‚â”‚
â”‚  â”‚                          â”‚ â€¢ Performance metrics          â”‚â”‚
â”‚  â”‚                          â”‚ â€¢ Alert notifications          â”‚â”‚
â”‚  â”‚                          â”‚ â€¢ Interactive data exploration  â”‚â”‚
â”‚  â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  Health Check Endpoints:                                        â”‚
â”‚  â€¢ NameNode:    http://localhost:9870/jmx                      â”‚
â”‚  â€¢ Spark:       http://localhost:8080/api/v1/status            â”‚
â”‚  â€¢ Dashboard:   http://localhost:8501/health                   â”‚
â”‚  â€¢ Kafka:       Internal broker health monitoring              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Network Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DOCKER NETWORK TOPOLOGY                   â”‚
â”‚                                                                 â”‚
â”‚  Host Machine (Windows/Linux)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Docker Internal Network                       â”‚â”‚
â”‚  â”‚              (hadoop-network: 172.20.0.0/16)             â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚â”‚
â”‚  â”‚  â”‚ NameNode    â”‚  â”‚ DataNode1   â”‚  â”‚ DataNode2   â”‚        â”‚â”‚
â”‚  â”‚  â”‚ 172.20.0.10 â”‚  â”‚ 172.20.0.11 â”‚  â”‚ 172.20.0.12 â”‚        â”‚â”‚
â”‚  â”‚  â”‚ Port: 9870  â”‚  â”‚ Port: 9864  â”‚  â”‚ Port: 9865  â”‚        â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚â”‚
â”‚  â”‚  â”‚ Spark       â”‚  â”‚ Kafka       â”‚  â”‚ Hive        â”‚        â”‚â”‚
â”‚  â”‚  â”‚ Master      â”‚  â”‚ Broker      â”‚  â”‚ Server      â”‚        â”‚â”‚
â”‚  â”‚  â”‚ 172.20.0.20 â”‚  â”‚ 172.20.0.30 â”‚  â”‚ 172.20.0.40 â”‚        â”‚â”‚
â”‚  â”‚  â”‚ Port: 8080  â”‚  â”‚ Port: 9092  â”‚  â”‚ Port: 10000 â”‚        â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚â”‚
â”‚  â”‚  â”‚ Dashboard   â”‚  â”‚ Scraper     â”‚  â”‚ Zookeeper   â”‚        â”‚â”‚
â”‚  â”‚  â”‚ 172.20.0.50 â”‚  â”‚ 172.20.0.60 â”‚  â”‚ 172.20.0.35 â”‚        â”‚â”‚
â”‚  â”‚  â”‚ Port: 8501  â”‚  â”‚ Internal    â”‚  â”‚ Port: 2181  â”‚        â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  Port Mappings (Host â†” Container):                             â”‚
â”‚  â€¢ 9870  â†” NameNode Web UI                                     â”‚
â”‚  â€¢ 8080  â†” Spark Master Web UI                                 â”‚
â”‚  â€¢ 8501  â†” Streamlit Dashboard                                 â”‚
â”‚  â€¢ 9864  â†” DataNode1 Web UI                                    â”‚
â”‚  â€¢ 9865  â†” DataNode2 Web UI                                    â”‚
â”‚  â€¢ 9000  â†” Kafka Manager (Optional)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Security Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SECURITY ARCHITECTURE                     â”‚
â”‚                                                                 â”‚
â”‚  Network Security                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Docker internal network isolation                        â”‚â”‚
â”‚  â”‚ â€¢ Port-based access control                                â”‚â”‚
â”‚  â”‚ â€¢ No external database connections                         â”‚â”‚
â”‚  â”‚ â€¢ Container-to-container communication only               â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  Data Security                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ HDFS block-level replication                             â”‚â”‚
â”‚  â”‚ â€¢ Data integrity checksums                                 â”‚â”‚
â”‚  â”‚ â€¢ Automatic backup and recovery                            â”‚â”‚
â”‚  â”‚ â€¢ Volume encryption (Docker volumes)                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  Access Control                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Web UI authentication (configurable)                     â”‚â”‚
â”‚  â”‚ â€¢ HDFS permission model                                    â”‚â”‚
â”‚  â”‚ â€¢ Hive table-level security                                â”‚â”‚
â”‚  â”‚ â€¢ API rate limiting for scrapers                           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This architecture provides:
- **Scalability**: Horizontal scaling through additional DataNodes and Spark workers
- **Fault Tolerance**: HDFS replication and Spark job recovery mechanisms
- **Real-time Processing**: Kafka streaming with Spark integration
- **Monitoring**: Comprehensive health checks and performance metrics
- **Security**: Network isolation and data protection
- **Flexibility**: Modular design allowing component replacement and extension

## âš¡ Quick Start

### Prerequisites

- **Git Bash** (mandatory for Windows users)
- Docker Desktop (with WSL2 on Windows)
- 8GB+ RAM recommended
- 20GB+ free disk space

### 1. Clone and Deploy

```bash
# In Git Bash terminal:
git clone https://github.com/hadoop-89/adam_hadoop.git
cd adam_hadoop

# Set permissions (MANDATORY STEP)
chmod +x scripts/*.sh

# Stop everything (all containers) and clean everything (volumes, unused images, cache, orphaned networks, etc.)
docker stop $(docker ps -aq) && docker system prune -af --volumes

# Deploy complete cluster
./scripts/deploy.sh
```

### 2. Verify Deployment

```bash
# Check cluster health
./scripts/deploy.sh --status

# View all services
docker ps

# Check logs if issues occur
docker-compose logs -f
```

### 3. Access Web Interfaces

- **NameNode UI**: http://localhost:9870
- **Spark Master**: http://localhost:8080  
- **Streamlit Dashboard**: http://localhost:8501
- **DataNode1**: http://localhost:9864
- **DataNode2**: http://localhost:9865
- **Kafka Manager**: http://localhost:9000

## ğŸŒ Real-Time Web Scraping

The scraper continuously collects data from multiple sources:

**Text Sources:**
- HackerNews RSS feeds
- Reddit Technology posts
- BBC Technology news
- TechCrunch articles
- Reuters Technology
- Stack Overflow trending

**Image Sources:**
- Reddit image subreddits (/r/earthporn, /r/cityporn, /r/spaceporn)
- Instagram public feeds (metadata only)
- Unsplash API integration
- Metadata extraction and categorization

**Data Flow:**
```
Web Sources â†’ Scraper â†’ Kafka â†’ Spark Streaming â†’ HDFS â†’ Hive â†’ Dashboard
```

**Scraping Configuration:**
```bash
# Adjust scraping frequency (default: 5 minutes)
export SCRAPE_INTERVAL=300

# Enable/disable specific sources
export ENABLE_REDDIT=true
export ENABLE_NEWS=true
export ENABLE_IMAGES=true
```

## ğŸ“Š Data Processing

### HDFS Structure
```
/data/
â”œâ”€â”€ text/
â”‚   â”œâ”€â”€ existing/     # Pre-loaded datasets (Amazon reviews, news)
â”‚   â”œâ”€â”€ scraped/      # Real-time scraped articles
â”‚   â””â”€â”€ processed/    # Cleaned and processed text
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ existing/     # Pre-loaded image metadata
â”‚   â”œâ”€â”€ scraped/      # Scraped image metadata
â”‚   â””â”€â”€ thumbnails/   # Generated thumbnails
â”œâ”€â”€ streaming/        # Real-time Kafka data
â”‚   â”œâ”€â”€ raw/         # Raw stream data
â”‚   â””â”€â”€ processed/   # Stream processing results
â”œâ”€â”€ analytics/       # Spark analytics results
â”œâ”€â”€ ia_results/      # AI analysis results
â””â”€â”€ backup/          # Backup and checkpoints
```

### Spark Analytics

```bash
# Run comprehensive analytics on collected data
docker exec spark-master python /opt/spark-jobs/demo_analytics.py

# Real-time streaming analytics
docker exec spark-master python /opt/spark-jobs/streaming_analytics.py

# Text sentiment analysis
docker exec spark-master python /opt/spark-jobs/sentiment_analysis.py

# Image metadata processing
docker exec spark-master python /opt/spark-jobs/image_processing.py

# Test Hadoop â†” AI integration
docker exec spark-master python /opt/spark-jobs/test_hadoop_ia_integration.py

# Custom analytics (modify parameters)
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 1g \
  --total-executor-cores 2 \
  /opt/spark-jobs/custom_analytics.py
```

### Hive Queries

```bash
# Connect to Hive
docker exec -it hive-server beeline -u jdbc:hive2://localhost:10000

# Example queries for text data
USE analytics;
SELECT source, COUNT(*) FROM reviews GROUP BY source;
SELECT category, AVG(rating) FROM reviews GROUP BY category;
SELECT DATE(created_at), COUNT(*) FROM scraped_articles 
  WHERE source='hackernews' GROUP BY DATE(created_at);

# Image metadata queries
SELECT image_category, COUNT(*) FROM image_metadata GROUP BY image_category;
SELECT AVG(file_size), source FROM image_metadata GROUP BY source;

# Streaming data analysis
SELECT hour, COUNT(*) FROM streaming_stats 
  WHERE date = CURRENT_DATE GROUP BY hour ORDER BY hour;

# Performance queries
ANALYZE TABLE reviews COMPUTE STATISTICS;
SHOW TABLE EXTENDED LIKE 'reviews';
```

## ğŸ¤– AI Integration

This Hadoop cluster is designed to work with the companion **AI API repository**:

```bash
# Test AI connectivity
curl http://localhost:8001/health

# Send text data to AI for sentiment analysis
python spark-jobs/api_client.py --type sentiment --data "sample text"

# Send image metadata for classification
python spark-jobs/api_client.py --type classification --data /path/to/image

# Batch processing integration
python spark-jobs/batch_ai_integration.py

# Real-time AI streaming
python spark-jobs/realtime_ai_stream.py
```

The AI integration enables:
- **Sentiment analysis** of scraped text data with real-time scoring
- **Image classification** of collected images using deep learning models
- **Topic modeling** for news articles and social media posts
- **Real-time ML inference** on streaming data with sub-second latency
- **Batch processing** for large datasets with distributed computing
- **Anomaly detection** in streaming data patterns
- **Recommendation systems** based on user behavior data

**AI Pipeline Features:**
- Automatic model retraining based on new data
- A/B testing for different AI models
- Model versioning and rollback capabilities
- Distributed inference across Spark cluster
- GPU acceleration support (if available)

## ğŸ“ˆ Monitoring & Dashboard

The Streamlit dashboard provides real-time insights:

**Main Dashboard Features:**
- **HDFS data statistics** and health metrics with visual charts
- **Real-time scraping activity** with live updates every 30 seconds
- **Kafka topic monitoring** and message counts with throughput graphs
- **Cluster health status** for all services with alert notifications
- **Performance metrics** and resource usage with historical trends
- **Data quality metrics** with validation reports
- **AI model performance** tracking and comparison

**Advanced Dashboard Pages:**
- **Data Explorer**: Interactive data browsing and filtering
- **Analytics Workbench**: Custom query interface
- **System Logs**: Centralized logging with search capabilities
- **Alert Center**: Real-time notifications and issue tracking
- **Resource Monitor**: CPU, memory, and disk usage across cluster

Features:
- Auto-refresh capabilities (configurable intervals)
- Interactive data exploration with drill-down capabilities
- Real-time scraped articles display with sentiment scores
- Image metadata visualization with thumbnail previews
- Export functionality for reports and data
- Dark/light theme support
- Mobile-responsive design

```bash
# Access dashboard components
# Main dashboard: http://localhost:8501
# Metrics API: http://localhost:8501/metrics
# Health check: http://localhost:8501/health
```

## ğŸ› ï¸ Configuration

### Environment Variables

Create `.env` file for custom configuration:

```bash
# Scraping Configuration
SCRAPE_INTERVAL=300
MAX_ARTICLES_PER_SOURCE=100
ENABLE_IMAGE_SCRAPING=true

# Kafka Configuration  
KAFKA_RETENTION_HOURS=168
KAFKA_PARTITIONS=3
KAFKA_REPLICATION_FACTOR=1

# Spark Configuration
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2
SPARK_EXECUTOR_MEMORY=1G

# HDFS Configuration
DFS_REPLICATION=2
DFS_BLOCKSIZE=134217728

# Dashboard Configuration
DASHBOARD_REFRESH_INTERVAL=30
ENABLE_ALERTS=true
LOG_LEVEL=INFO
```

### Scaling DataNodes

Modify `docker-compose.yml` to add more DataNodes:

```yaml
datanode3:
  build:
    context: ./hadoop-datanode
  container_name: datanode3
  hostname: datanode3
  ports:
    - "9866:9864"
    - "9876:9866"
  volumes:
    - datanode3-data:/hadoop/dfs/data
  environment:
    - CLUSTER_NAME=hadoop-cluster
  networks:
    - hadoop-network

volumes:
  datanode3-data:
```

### Scraper Configuration

Advanced scraper settings in `scraper/config.py`:

```python
# Source-specific configurations
REDDIT_CONFIG = {
    'subreddits': ['technology', 'programming', 'MachineLearning'],
    'limit': 50,
    'time_filter': 'day'
}

NEWS_CONFIG = {
    'sources': ['bbc', 'techcrunch', 'reuters'],
    'categories': ['technology', 'business', 'science'],
    'language': 'en'
}

IMAGE_CONFIG = {
    'min_resolution': (800, 600),
    'max_file_size': '10MB',
    'allowed_formats': ['jpg', 'png', 'webp']
}
```

### Kafka Topics Management

```bash
# List current topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Create new topic with custom partitions
docker exec kafka kafka-topics --bootstrap-server localhost:9092 \
  --create --topic custom-topic --partitions 6 --replication-factor 1

# Modify topic configuration
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics --entity-name scraped-data \
  --alter --add-config retention.ms=604800000

# Monitor topic performance
docker exec kafka kafka-consumer-perf-test \
  --bootstrap-server localhost:9092 --topic scraped-data \
  --messages 1000 --threads 1
```

## ğŸ”§ Advanced Operations

### Cluster Management

```bash
# Clean restart (preserves data)
./scripts/deploy.sh --clean

# Fresh deployment (removes all data - USE WITH CAUTION)
./scripts/deploy.sh --fresh

# Debug mode with verbose logging
./scripts/deploy.sh --debug

# Ordered deployment for troubleshooting
./scripts/deploy.sh --ordered

# Scale specific services
./scripts/deploy.sh --scale spark-worker=3

# Backup cluster state
./scripts/backup.sh --full

# Restore from backup
./scripts/restore.sh --backup-id 20231215_143022
```

### Data Loading and Migration

```bash
# Load external datasets
docker-compose run --rm data-loader

# Bulk data import from local files
docker exec namenode hdfs dfs -put /local/data/* /data/external/

# Export data for external processing
docker exec namenode hdfs dfs -get /data/processed /local/export/

# Data migration between clusters
./scripts/migrate_data.sh --source cluster1 --target cluster2

# Incremental backup
./scripts/backup.sh --incremental --since yesterday
```

### Performance Optimization

```bash
# Optimize HDFS blocks
docker exec namenode hdfs balancer -threshold 5

# Compact Hive tables
docker exec hive-server hive -e "ALTER TABLE reviews COMPACT 'major';"

# Spark performance tuning
docker exec spark-master spark-submit \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  /opt/spark-jobs/optimized_job.py

# Monitor cluster performance
./scripts/performance_monitor.sh --duration 3600
```

## ğŸš¨ Troubleshooting

### Common Issues and Solutions

**NameNode Safe Mode Issues:**
```bash
# Check safe mode status
docker exec namenode hdfs dfsadmin -safemode get

# Force leave safe mode (use carefully)
docker exec namenode hdfs dfsadmin -safemode leave

# Check block reports
docker exec namenode hdfs dfsadmin -report
```

**DataNodes Not Connecting:**
```bash
# Verify network connectivity
docker exec namenode hdfs dfsadmin -report

# Check DataNode logs
docker logs datanode1 --tail 50
docker logs datanode2 --tail 50

# Restart specific DataNode
docker-compose restart datanode1

# Check HDFS health
docker exec namenode hdfs fsck / -files -blocks -locations
```

**Kafka Connectivity Issues:**
```bash
# Test Kafka broker connectivity
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Check consumer lag
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 --describe --all-groups

# Reset consumer group (if stuck)
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 --group scraper-group --reset-offsets \
  --to-earliest --topic scraped-data --execute

# Monitor Kafka performance
docker exec kafka kafka-log-dirs \
  --bootstrap-server localhost:9092 --describe --json
```

**Spark Job Failures:**
```bash
# Check Spark application logs
docker exec spark-master spark-submit \
  --deploy-mode client --driver-java-options "-Dlog4j.debug=true" \
  /opt/spark-jobs/failing_job.py

# Monitor Spark resources
curl http://localhost:8080/api/v1/applications

# Clear Spark checkpoints
docker exec spark-master rm -rf /tmp/spark-checkpoints/*

# Restart Spark cluster
docker-compose restart spark-master spark-worker1 spark-worker2
```

**Dashboard Not Loading:**
```bash
# Check Streamlit logs
docker logs dashboard --tail 50

# Verify port availability
netstat -an | grep 8501

# Restart dashboard with debug mode
docker-compose up dashboard --build

# Test dashboard health endpoint
curl http://localhost:8501/health
```

### Health Checks and Monitoring

```bash
# Complete cluster health check
./scripts/health_check.sh --comprehensive

# Individual service health
curl http://localhost:9870/jmx              # NameNode JMX
curl http://localhost:8501/metrics          # Dashboard metrics
curl http://localhost:8080/api/v1/status    # Spark status

# Resource usage monitoring
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

# Disk usage analysis
docker exec namenode hdfs dfs -du -h /data
docker system df -v
```

### Log Analysis

```bash
# Centralized log collection
./scripts/collect_logs.sh --output /tmp/cluster_logs/

# Search across all logs
./scripts/search_logs.sh --pattern "ERROR" --since "2023-12-15"

# Real-time log monitoring
docker-compose logs -f --tail 10 namenode datanode1 spark-master

# Export logs for analysis
./scripts/export_logs.sh --format json --timerange last_hour
```

## ğŸ“Š Performance Tuning

### Resource Allocation

Optimize Docker resource allocation in `docker-compose.yml`:

```yaml
spark-master:
  environment:
    - SPARK_WORKER_MEMORY=4G
    - SPARK_WORKER_CORES=4
    - SPARK_DRIVER_MEMORY=2G
  deploy:
    resources:
      limits:
        memory: 6G
        cpus: '4.0'
      reservations:
        memory: 4G
        cpus: '2.0'
```

### HDFS Configuration

Optimize HDFS performance in `hadoop-namenode/hdfs-site.xml`:

```xml
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
<property>
    <name>dfs.block.size</name>
    <value>268435456</value> <!-- 256MB blocks -->
</property>
<property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>8192</value>
</property>
```

### Kafka Performance Tuning

```bash
# Optimize Kafka for throughput
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type brokers --entity-name 1 \
  --alter --add-config num.network.threads=8,num.io.threads=16

# Batch processing optimization
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics --entity-name scraped-data \
  --alter --add-config batch.size=32768,linger.ms=10
```

## ğŸ”— Integration Points

This Hadoop cluster integrates with:

- **AI API Repository**: RESTful ML inference and batch processing
- **External data sources**: Real-time web scraping with rate limiting
- **Analytics tools**: Jupyter notebooks, Tableau, Power BI connectors
- **Monitoring systems**: Prometheus, Grafana, ELK stack integration
- **Cloud platforms**: AWS S3, Azure Blob, Google Cloud Storage
- **CI/CD pipelines**: Jenkins, GitLab CI, GitHub Actions
- **Message queues**: RabbitMQ, Apache Pulsar compatibility

### API Endpoints

```bash
# Cluster status API
GET http://localhost:9870/webhdfs/v1/?op=LISTSTATUS

# Spark job submission API  
POST http://localhost:8080/api/v1/submissions/create

# Dashboard API
GET http://localhost:8501/api/metrics
GET http://localhost:8501/api/data/recent

# Health check endpoints
GET http://localhost:9870/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState
```

## ğŸ“š Documentation

### Official Documentation
- [Hadoop Official Documentation](https://hadoop.apache.org/docs/)
- [Spark Programming Guide](https://spark.apache.org/docs/latest/)
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Hive User Guide](https://cwiki.apache.org/confluence/display/Hive/Home)

### Project-Specific Guides
- [Deployment Guide](./docs/deployment.md)
- [Troubleshooting Guide](./docs/troubleshooting.md)
- [API Reference](./docs/api_reference.md)
- [Performance Tuning](./docs/performance.md)

## ğŸ† Project Structure

```
adam_hadoop/
â”œâ”€â”€ ansible/                 # Automated deployment and configuration
â”‚   â”œâ”€â”€ playbooks/          # Ansible playbooks for cluster setup
â”‚   â”œâ”€â”€ inventory/          # Environment configurations
â”‚   â””â”€â”€ roles/              # Reusable Ansible roles
â”œâ”€â”€ dashboard/               # Streamlit monitoring dashboard  
â”‚   â”œâ”€â”€ pages/              # Multi-page dashboard components
â”‚   â”œâ”€â”€ components/         # Reusable UI components
â”‚   â”œâ”€â”€ utils/              # Dashboard utilities
â”‚   â””â”€â”€ app.py              # Main dashboard application
â”œâ”€â”€ data-loader/            # External dataset loading
â”‚   â”œâ”€â”€ loaders/            # Different data source loaders
â”‚   â”œâ”€â”€ transformers/       # Data transformation scripts
â”‚   â””â”€â”€ validators/         # Data quality validation
â”œâ”€â”€ docs/                   # Comprehensive documentation
â”‚   â”œâ”€â”€ api/                # API documentation
â”‚   â”œâ”€â”€ deployment/         # Deployment guides
â”‚   â””â”€â”€ troubleshooting/    # Issue resolution guides
â”œâ”€â”€ hadoop-namenode/        # NameNode configuration
â”‚   â”œâ”€â”€ config/             # Hadoop configuration files
â”‚   â””â”€â”€ scripts/            # NameNode-specific scripts
â”œâ”€â”€ hadoop-datanode/        # DataNode configuration
â”‚   â”œâ”€â”€ config/             # DataNode configurations
â”‚   â””â”€â”€ scripts/            # DataNode-specific scripts
â”œâ”€â”€ hive-config/            # Hive warehouse setup
â”‚   â”œâ”€â”€ schemas/            # Database schemas
â”‚   â”œâ”€â”€ tables/             # Table definitions
â”‚   â””â”€â”€ queries/            # Sample queries
â”œâ”€â”€ monitoring/             # Monitoring and alerting
â”‚   â”œâ”€â”€ prometheus/         # Prometheus configuration
â”‚   â”œâ”€â”€ grafana/           # Grafana dashboards
â”‚   â””â”€â”€ alerts/            # Alert configurations
â”œâ”€â”€ scraper/                # Real-time web scraping
â”‚   â”œâ”€â”€ sources/            # Different data source scrapers
â”‚   â”œâ”€â”€ processors/         # Data processing modules
â”‚   â””â”€â”€ config/             # Scraper configurations
â”œâ”€â”€ spark-jobs/             # Spark analytics jobs
â”‚   â”œâ”€â”€ batch/              # Batch processing jobs
â”‚   â”œâ”€â”€ streaming/          # Real-time streaming jobs
â”‚   â”œâ”€â”€ ml/                 # Machine learning pipelines
â”‚   â””â”€â”€ utils/              # Common utilities
â”œâ”€â”€ scripts/                # Deployment and management scripts
â”‚   â”œâ”€â”€ deploy.sh           # Main deployment script
â”‚   â”œâ”€â”€ backup.sh           # Backup utilities
â”‚   â”œâ”€â”€ restore.sh          # Restore utilities
â”‚   â””â”€â”€ health_check.sh     # Health monitoring
â”œâ”€â”€ tests/                  # Test suites
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â”œâ”€â”€ integration/        # Integration tests
â”‚   â””â”€â”€ performance/        # Performance tests
â”œâ”€â”€ docker-compose.yml      # Complete service orchestration
â”œâ”€â”€ docker-compose.dev.yml  # Development environment
â”œâ”€â”€ docker-compose.prod.yml # Production environment
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ Makefile               # Build and management commands
â””â”€â”€ README.md              # This comprehensive guide
```

## ğŸ¯ Use Cases

This infrastructure supports diverse applications:

### Academic and Research
- **Big data research projects** with distributed computing capabilities
- **Machine learning experiments** with large-scale datasets
- **Data mining studies** on social media and news content
- **Performance benchmarking** of big data technologies
- **Educational demonstrations** of Hadoop ecosystem components

### Business Applications
- **Real-time analytics** for streaming web content and social media
- **Content aggregation** for news and media companies
- **Market sentiment analysis** from social media and news sources
- **Data warehousing** for business intelligence and reporting
- **ETL pipelines** for data integration and transformation

### Technical Applications  
- **Proof of concept** for big data architectures
- **Development platform** for Spark and Hadoop applications
- **Testing environment** for data processing algorithms
- **Integration platform** for AI and ML workflows
- **Monitoring solution** for distributed systems

### Advanced Use Cases
- **Real-time recommendation systems** based on user behavior
- **Fraud detection** using streaming analytics
- **IoT data processing** for sensor data analysis
- **Log analysis** for security and compliance monitoring
- **A/B testing platform** for data-driven decision making

## ğŸ¤ Contributing

We welcome contributions to improve this Hadoop infrastructure:

### Getting Started
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Set up development environment (`./scripts/setup_dev.sh`)
4. Make your changes with comprehensive tests
5. Commit changes (`git commit -m 'Add amazing feature'`)
6. Push to branch (`git push origin feature/amazing-feature`)
7. Open Pull Request with detailed description

### Contribution Guidelines
- Follow existing code style and conventions
- Add comprehensive tests for new features
- Update documentation for any changes
- Ensure all tests pass before submitting PR
- Include performance impact analysis for major changes

### Areas for Contribution
- Additional data source scrapers
- New Spark analytics jobs
- Dashboard improvements and new visualizations
- Performance optimizations
- Documentation enhancements
- Test coverage improvements
- Security enhancements
- Cloud deployment templates

## ğŸ“„ License

This project is part of an academic assignment and is intended for educational purposes. 

**License Terms:**
- Educational use permitted
- Commercial use requires permission
- Attribution required for academic papers
- No warranty provided
- Subject to university academic integrity policies

## ğŸš€ What's Next?

### Planned Enhancements
- **Multi-machine cluster deployment** with Kubernetes orchestration
- **Elasticsearch integration** for advanced search and analytics capabilities
- **Cloud storage integration** (AWS S3, Azure Blob Storage, Google Cloud Storage)
- **Advanced ML pipelines** with MLflow experiment tracking
- **Real-time alerting system** with Slack/Teams integration
- **Security enhancements** with Kerberos authentication
- **Data governance** with Apache Atlas integration
- **Stream processing** with Apache Flink integration

### Performance Improvements
- **GPU acceleration** for AI workloads
- **Columnar storage** with Apache Parquet optimization
- **Caching layer** with Redis for frequently accessed data
- **Load balancing** for high-availability deployments
- **Auto-scaling** based on workload demands

### Monitoring and Observability
- **Distributed tracing** with Jaeger
- **Advanced metrics** with Prometheus and Grafana
- **Log aggregation** with ELK stack
- **Anomaly detection** in system metrics
- **Capacity planning** tools and forecasting

---

**ğŸ“ Ready to explore big data? Follow the prerequisites above, then deploy your cluster with `./scripts/deploy.sh` and start your data journey! ğŸš€**

**ğŸ“§ For questions or issues, please create a GitHub issue with detailed logs and system information.**
