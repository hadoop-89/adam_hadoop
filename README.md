# ğŸš€ Projet Hadoop - Cluster & Traitement de DonnÃ©es  

## ğŸ“Œ Description  
Ce projet met en place un cluster Hadoop sous Docker avec 1 NameNode et 2 DataNodes. Il intÃ¨gre Ã©galement un pipeline de chargement de donnÃ©es depuis Kaggle vers HDFS, facilitant ainsi le traitement de donnÃ©es texte et image via Hadoop et Spark.  

## ğŸ—ï¸ Technologies utilisÃ©es  
- **Hadoop 3.2.1** (HDFS)  
- **Docker & Docker-Compose**  
- **Ansible** (automatisation)  
- **Kaggle API** (import de datasets)  
- **Spark / Hive** (traitement des donnÃ©es)  

## ğŸ“‚ Architecture  
Le cluster est constituÃ© de trois conteneurs Docker :  
- ğŸ–¥ï¸ **NameNode** : Gestion du systÃ¨me de fichiers distribuÃ©  
- ğŸ“¦ **DataNode1 & DataNode2** : Stockage et traitement des donnÃ©es  

## ğŸš€ Installation et DÃ©marrage  

### 1ï¸âƒ£ PrÃ©requis  
Avant de commencer, assurez-vous d'avoir :
- **WSL2 + Ubuntu** installÃ© sous Windows
- **Docker Desktop** configurÃ© avec WSL
- **Kaggle CLI** installÃ© (`pip install kaggle`)
- Des identifiants Kaggle disponibles via un fichier `~/.kaggle/kaggle.json` ou
  les variables d'environnement `KAGGLE_USERNAME` et `KAGGLE_KEY`

### 2ï¸âƒ£ DÃ©marrer le cluster Hadoop  
```bash  
docker-compose up -d  
```  
Cela lance les conteneurs Hadoop en arriÃ¨re-plan.  

### 3ï¸âƒ£ VÃ©rifier l'Ã©tat du cluster  
```bash  
docker ps  
```  
Vous devriez voir `namenode`, `datanode1` et `datanode2` en cours d'exÃ©cution.  

### 4ï¸âƒ£ Charger les bases de donnÃ©es Kaggle
Assurez-vous que vos identifiants Kaggle sont disponibles (fichier `kaggle.json`
montÃ© dans le conteneur ou variables `KAGGLE_USERNAME` et `KAGGLE_KEY`).
Lancer ensuite le script d'importation :
```bash  
chmod +x load_db_kaggle.sh  
./load_db_kaggle.sh  
```  
Cela va :  
âœ”ï¸ TÃ©lÃ©charger les datasets (textes et images) depuis Kaggle  
âœ”ï¸ Extraire et stocker les fichiers en local  
âœ”ï¸ Copier les donnÃ©es dans HDFS  

## ğŸ” AccÃ¨s aux interfaces  
- **Interface HDFS NameNode** : [localhost:9870](http://localhost:9870)  
- **Spark UI (si activÃ©)** : [localhost:8080](http://localhost:8080)  
- **Hive Metastore (si configurÃ©)** : Port 10000  

## ğŸ“Œ Prochaines Ã‰tapes  
âœ”ï¸ IntÃ©gration Spark/Hive pour lâ€™analyse des donnÃ©es  
âœ”ï¸ Mise en place dâ€™un flux Kafka pour ingestion temps rÃ©el  
âœ”ï¸ Ajout dâ€™une API Flask pour traitement IA avec YOLO  

## ğŸ› ï¸ DÃ©veloppement  
Clonez le projet et modifiez `docker-compose.yml` ou `load_db_kaggle.sh` pour adapter le cluster et les datasets.  
```bash  
git clone https://github.com/votre-repo/projet-hadoop.git  
cd projet-hadoop  
```  
