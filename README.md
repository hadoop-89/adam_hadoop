# ğŸš€ Projet Hadoop - Cluster & Traitement de DonnÃ©es  

## ğŸ“Œ Description  
Ce projet met en place un cluster Hadoop sous Docker avec 1 NameNode et 2 DataNodes. Il intÃ¨gre Ã©galement un pipeline de chargement de donnÃ©es depuis Kaggle vers HDFS, facilitant ainsi le traitement de donnÃ©es texte et image via Hadoop et Spark.  

## ğŸ—ï¸ Technologies utilisÃ©es  
- **Hadoop 3.2.1** (HDFS)  
- **Docker & Docker-Compose**  
- **Ansible** (automatisation)  
- **Kaggle API** (import de datasets)  
- **Spark / Hive** (traitement des donnÃ©es)  

## ğŸ“‚ Architecture  
Le cluster est constituÃ© de trois conteneurs Docker :  
- ğŸ–¥ï¸ **NameNode** : Gestion du systÃ¨me de fichiers distribuÃ©  
- ğŸ“¦ **DataNode1 & DataNode2** : Stockage et traitement des donnÃ©es  

## ğŸš€ Installation et DÃ©marrage  

### 1ï¸âƒ£ PrÃ©requis  
Avant de commencer, assurez-vous d'avoir :
- **WSL2 + Ubuntu** installÃ© sous Windows
- **Docker Desktop** configurÃ© avec WSL
- **Kaggle CLI** installÃ© (`pip install kaggle`)
- Des identifiants Kaggle disponibles via un fichier `~/.kaggle/kaggle.json` ou
  les variables d'environnement `KAGGLE_USERNAME` et `KAGGLE_KEY`

### 2ï¸âƒ£ DÃ©marrer le cluster Hadoop
Lancez le script suivant qui dÃ©marre l'ensemble des conteneurs et initialise HDFS :
```bash
./scripts/deploy.sh
```
Le script peut Ãªtre relancÃ© avec `--clean` pour redÃ©marrer proprement le cluster.

### 3ï¸âƒ£ VÃ©rifier l'Ã©tat du cluster  
```bash  
docker ps  
```  
Vous devriez voir `namenode`, `datanode1` et `datanode2` en cours d'exÃ©cution.  

### 4ï¸âƒ£ Charger les bases de donnÃ©es Kaggle
Le script `deploy.sh` exÃ©cute automatiquement le conteneur `data-loader` pour
importer les jeux de donnÃ©es si vos identifiants Kaggle sont fournis (fichier
`~/.kaggle/kaggle.json` montÃ© ou variables `KAGGLE_USERNAME` et `KAGGLE_KEY`).

Pour relancer manuellement l'import :
```bash
docker-compose run --rm -v ~/.kaggle:/root/.kaggle \
    -e KAGGLE_USERNAME -e KAGGLE_KEY data-loader
```

## ğŸ” AccÃ¨s aux interfaces  
- **Interface HDFS NameNode** : [localhost:9870](http://localhost:9870)  
- **Spark UI (si activÃ©)** : [localhost:8080](http://localhost:8080)  
- **Hive Metastore (si configurÃ©)** : Port 10000  

## ğŸ“Œ Prochaines Ã‰tapes  
âœ”ï¸ IntÃ©gration Spark/Hive pour lâ€™analyse des donnÃ©es  
âœ”ï¸ Mise en place dâ€™un flux Kafka pour ingestion temps rÃ©el  
âœ”ï¸ Ajout dâ€™une API Flask pour traitement IA avec YOLO  

## ğŸ› ï¸ DÃ©veloppement  
Clonez le projet et modifiez `docker-compose.yml` ou `data-loader/load_db_hdfs.sh` pour adapter le cluster et les datasets.
```bash  
git clone https://github.com/votre-repo/projet-hadoop.git  
cd projet-hadoop  
```  
