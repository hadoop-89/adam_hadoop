# ğŸš€ Hadoop Big Data Infrastructure

**Complete Hadoop cluster with real-time web scraping, data processing, and AI integration**

This repository contains the Hadoop infrastructure component of our **Hadoop & AI Project**, designed to handle big data processing, real-time web scraping, and seamless integration with AI services.

## âš ï¸ Important Prerequisites for Evaluators

**Before starting, please ensure the following setup:**

### 1. Install Git Bash (Required)
This project uses shell scripts that require Git Bash on Windows:

```bash
# Download and install Git for Windows from:
# https://git-scm.com/download/win

# After installation, always use Git Bash terminal for this project
```

### 2. Set File Permissions (Critical)
After cloning, you **must** give execution rights to script files:

```bash
# Navigate to project directory in Git Bash
cd adam_hadoop

# Grant execution permissions to all scripts
chmod +x scripts/*.sh
chmod +x *.sh
find . -name "*.sh" -exec chmod +x {} \;

# Verify permissions
ls -la scripts/
```

### 3. Docker Desktop Setup
```bash
# Ensure Docker Desktop is running with WSL2 backend
# Minimum requirements:
# - 8GB RAM allocated to Docker
# - 20GB free disk space
# - WSL2 enabled on Windows
```

**âš¡ Without these prerequisites, the deployment will fail!**

---

## ğŸ“‹ Overview

This project implements a production-ready Hadoop ecosystem that includes:

- **Multi-node Hadoop cluster** (1 NameNode + 2 DataNodes)
- **Real-time web scraping** with Kafka streaming
- **Spark processing** for data analytics
- **Hive data warehouse** for structured queries
- **Streamlit dashboard** for real-time monitoring
- **AI API integration** for machine learning workflows

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Sources   â”‚â”€â”€â”€â–¶â”‚   Web Scraper   â”‚â”€â”€â”€â–¶â”‚      Kafka      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚â—€â”€â”€â”€â”‚  Spark Cluster  â”‚â—€â”€â”€â”€â”‚   Streaming     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Hive/Analyticsâ”‚â—€â”€â”€â”€â”‚  HDFS Storage   â”‚â—€â”€â”€â”€â”‚   Data Loading   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   AI API        â”‚
                    â”‚  Integration    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš¡ Quick Start

### Prerequisites

- **Git Bash** (mandatory for Windows users)
- Docker Desktop (with WSL2 on Windows)
- 8GB+ RAM recommended
- 20GB+ free disk space

### 1. Clone and Deploy

```bash
# In Git Bash terminal:
git clone https://github.com/hadoop-89/adam_hadoop.git
cd adam_hadoop

# Set permissions (MANDATORY STEP)
chmod +x scripts/*.sh

# Deploy complete cluster
./scripts/deploy.sh
```

### 2. Verify Deployment

```bash
# Check cluster health
./scripts/deploy.sh --status

# View all services
docker ps

# Check logs if issues occur
docker-compose logs -f
```

### 3. Access Web Interfaces

- **NameNode UI**: http://localhost:9870
- **Spark Master**: http://localhost:8080  
- **Streamlit Dashboard**: http://localhost:8501
- **DataNode1**: http://localhost:9864
- **DataNode2**: http://localhost:9865
- **Kafka Manager**: http://localhost:9000

## ğŸŒ Real-Time Web Scraping

The scraper continuously collects data from multiple sources:

**Text Sources:**
- HackerNews RSS feeds
- Reddit Technology posts
- BBC Technology news
- TechCrunch articles
- Reuters Technology
- Stack Overflow trending

**Image Sources:**
- Reddit image subreddits (/r/earthporn, /r/cityporn, /r/spaceporn)
- Instagram public feeds (metadata only)
- Unsplash API integration
- Metadata extraction and categorization

**Data Flow:**
```
Web Sources â†’ Scraper â†’ Kafka â†’ Spark Streaming â†’ HDFS â†’ Hive â†’ Dashboard
```

**Scraping Configuration:**
```bash
# Adjust scraping frequency (default: 5 minutes)
export SCRAPE_INTERVAL=300

# Enable/disable specific sources
export ENABLE_REDDIT=true
export ENABLE_NEWS=true
export ENABLE_IMAGES=true
```

## ğŸ“Š Data Processing

### HDFS Structure
```
/data/
â”œâ”€â”€ text/
â”‚   â”œâ”€â”€ existing/     # Pre-loaded datasets (Amazon reviews, news)
â”‚   â”œâ”€â”€ scraped/      # Real-time scraped articles
â”‚   â””â”€â”€ processed/    # Cleaned and processed text
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ existing/     # Pre-loaded image metadata
â”‚   â”œâ”€â”€ scraped/      # Scraped image metadata
â”‚   â””â”€â”€ thumbnails/   # Generated thumbnails
â”œâ”€â”€ streaming/        # Real-time Kafka data
â”‚   â”œâ”€â”€ raw/         # Raw stream data
â”‚   â””â”€â”€ processed/   # Stream processing results
â”œâ”€â”€ analytics/       # Spark analytics results
â”œâ”€â”€ ia_results/      # AI analysis results
â””â”€â”€ backup/          # Backup and checkpoints
```

### Spark Analytics

```bash
# Run comprehensive analytics on collected data
docker exec spark-master python /opt/spark-jobs/demo_analytics.py

# Real-time streaming analytics
docker exec spark-master python /opt/spark-jobs/streaming_analytics.py

# Text sentiment analysis
docker exec spark-master python /opt/spark-jobs/sentiment_analysis.py

# Image metadata processing
docker exec spark-master python /opt/spark-jobs/image_processing.py

# Test Hadoop â†” AI integration
docker exec spark-master python /opt/spark-jobs/test_hadoop_ia_integration.py

# Custom analytics (modify parameters)
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 1g \
  --total-executor-cores 2 \
  /opt/spark-jobs/custom_analytics.py
```

### Hive Queries

```bash
# Connect to Hive
docker exec -it hive-server beeline -u jdbc:hive2://localhost:10000

# Example queries for text data
USE analytics;
SELECT source, COUNT(*) FROM reviews GROUP BY source;
SELECT category, AVG(rating) FROM reviews GROUP BY category;
SELECT DATE(created_at), COUNT(*) FROM scraped_articles 
  WHERE source='hackernews' GROUP BY DATE(created_at);

# Image metadata queries
SELECT image_category, COUNT(*) FROM image_metadata GROUP BY image_category;
SELECT AVG(file_size), source FROM image_metadata GROUP BY source;

# Streaming data analysis
SELECT hour, COUNT(*) FROM streaming_stats 
  WHERE date = CURRENT_DATE GROUP BY hour ORDER BY hour;

# Performance queries
ANALYZE TABLE reviews COMPUTE STATISTICS;
SHOW TABLE EXTENDED LIKE 'reviews';
```

## ğŸ¤– AI Integration

This Hadoop cluster is designed to work with the companion **AI API repository**:

```bash
# Test AI connectivity
curl http://localhost:8001/health

# Send text data to AI for sentiment analysis
python spark-jobs/api_client.py --type sentiment --data "sample text"

# Send image metadata for classification
python spark-jobs/api_client.py --type classification --data /path/to/image

# Batch processing integration
python spark-jobs/batch_ai_integration.py

# Real-time AI streaming
python spark-jobs/realtime_ai_stream.py
```

The AI integration enables:
- **Sentiment analysis** of scraped text data with real-time scoring
- **Image classification** of collected images using deep learning models
- **Topic modeling** for news articles and social media posts
- **Real-time ML inference** on streaming data with sub-second latency
- **Batch processing** for large datasets with distributed computing
- **Anomaly detection** in streaming data patterns
- **Recommendation systems** based on user behavior data

**AI Pipeline Features:**
- Automatic model retraining based on new data
- A/B testing for different AI models
- Model versioning and rollback capabilities
- Distributed inference across Spark cluster
- GPU acceleration support (if available)

## ğŸ“ˆ Monitoring & Dashboard

The Streamlit dashboard provides real-time insights:

**Main Dashboard Features:**
- **HDFS data statistics** and health metrics with visual charts
- **Real-time scraping activity** with live updates every 30 seconds
- **Kafka topic monitoring** and message counts with throughput graphs
- **Cluster health status** for all services with alert notifications
- **Performance metrics** and resource usage with historical trends
- **Data quality metrics** with validation reports
- **AI model performance** tracking and comparison

**Advanced Dashboard Pages:**
- **Data Explorer**: Interactive data browsing and filtering
- **Analytics Workbench**: Custom query interface
- **System Logs**: Centralized logging with search capabilities
- **Alert Center**: Real-time notifications and issue tracking
- **Resource Monitor**: CPU, memory, and disk usage across cluster

Features:
- Auto-refresh capabilities (configurable intervals)
- Interactive data exploration with drill-down capabilities
- Real-time scraped articles display with sentiment scores
- Image metadata visualization with thumbnail previews
- Export functionality for reports and data
- Dark/light theme support
- Mobile-responsive design

```bash
# Access dashboard components
# Main dashboard: http://localhost:8501
# Metrics API: http://localhost:8501/metrics
# Health check: http://localhost:8501/health
```

## ğŸ› ï¸ Configuration

### Environment Variables

Create `.env` file for custom configuration:

```bash
# Scraping Configuration
SCRAPE_INTERVAL=300
MAX_ARTICLES_PER_SOURCE=100
ENABLE_IMAGE_SCRAPING=true

# Kafka Configuration  
KAFKA_RETENTION_HOURS=168
KAFKA_PARTITIONS=3
KAFKA_REPLICATION_FACTOR=1

# Spark Configuration
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2
SPARK_EXECUTOR_MEMORY=1G

# HDFS Configuration
DFS_REPLICATION=2
DFS_BLOCKSIZE=134217728

# Dashboard Configuration
DASHBOARD_REFRESH_INTERVAL=30
ENABLE_ALERTS=true
LOG_LEVEL=INFO
```

### Scaling DataNodes

Modify `docker-compose.yml` to add more DataNodes:

```yaml
datanode3:
  build:
    context: ./hadoop-datanode
  container_name: datanode3
  hostname: datanode3
  ports:
    - "9866:9864"
    - "9876:9866"
  volumes:
    - datanode3-data:/hadoop/dfs/data
  environment:
    - CLUSTER_NAME=hadoop-cluster
  networks:
    - hadoop-network

volumes:
  datanode3-data:
```

### Scraper Configuration

Advanced scraper settings in `scraper/config.py`:

```python
# Source-specific configurations
REDDIT_CONFIG = {
    'subreddits': ['technology', 'programming', 'MachineLearning'],
    'limit': 50,
    'time_filter': 'day'
}

NEWS_CONFIG = {
    'sources': ['bbc', 'techcrunch', 'reuters'],
    'categories': ['technology', 'business', 'science'],
    'language': 'en'
}

IMAGE_CONFIG = {
    'min_resolution': (800, 600),
    'max_file_size': '10MB',
    'allowed_formats': ['jpg', 'png', 'webp']
}
```

### Kafka Topics Management

```bash
# List current topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Create new topic with custom partitions
docker exec kafka kafka-topics --bootstrap-server localhost:9092 \
  --create --topic custom-topic --partitions 6 --replication-factor 1

# Modify topic configuration
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics --entity-name scraped-data \
  --alter --add-config retention.ms=604800000

# Monitor topic performance
docker exec kafka kafka-consumer-perf-test \
  --bootstrap-server localhost:9092 --topic scraped-data \
  --messages 1000 --threads 1
```

## ğŸ”§ Advanced Operations

### Cluster Management

```bash
# Clean restart (preserves data)
./scripts/deploy.sh --clean

# Fresh deployment (removes all data - USE WITH CAUTION)
./scripts/deploy.sh --fresh

# Debug mode with verbose logging
./scripts/deploy.sh --debug

# Ordered deployment for troubleshooting
./scripts/deploy.sh --ordered

# Scale specific services
./scripts/deploy.sh --scale spark-worker=3

# Backup cluster state
./scripts/backup.sh --full

# Restore from backup
./scripts/restore.sh --backup-id 20231215_143022
```

### Data Loading and Migration

```bash
# Load external datasets
docker-compose run --rm data-loader

# Bulk data import from local files
docker exec namenode hdfs dfs -put /local/data/* /data/external/

# Export data for external processing
docker exec namenode hdfs dfs -get /data/processed /local/export/

# Data migration between clusters
./scripts/migrate_data.sh --source cluster1 --target cluster2

# Incremental backup
./scripts/backup.sh --incremental --since yesterday
```

### Performance Optimization

```bash
# Optimize HDFS blocks
docker exec namenode hdfs balancer -threshold 5

# Compact Hive tables
docker exec hive-server hive -e "ALTER TABLE reviews COMPACT 'major';"

# Spark performance tuning
docker exec spark-master spark-submit \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.adaptive.skewJoin.enabled=true \
  /opt/spark-jobs/optimized_job.py

# Monitor cluster performance
./scripts/performance_monitor.sh --duration 3600
```

## ğŸš¨ Troubleshooting

### Common Issues and Solutions

**NameNode Safe Mode Issues:**
```bash
# Check safe mode status
docker exec namenode hdfs dfsadmin -safemode get

# Force leave safe mode (use carefully)
docker exec namenode hdfs dfsadmin -safemode leave

# Check block reports
docker exec namenode hdfs dfsadmin -report
```

**DataNodes Not Connecting:**
```bash
# Verify network connectivity
docker exec namenode hdfs dfsadmin -report

# Check DataNode logs
docker logs datanode1 --tail 50
docker logs datanode2 --tail 50

# Restart specific DataNode
docker-compose restart datanode1

# Check HDFS health
docker exec namenode hdfs fsck / -files -blocks -locations
```

**Kafka Connectivity Issues:**
```bash
# Test Kafka broker connectivity
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Check consumer lag
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 --describe --all-groups

# Reset consumer group (if stuck)
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 --group scraper-group --reset-offsets \
  --to-earliest --topic scraped-data --execute

# Monitor Kafka performance
docker exec kafka kafka-log-dirs \
  --bootstrap-server localhost:9092 --describe --json
```

**Spark Job Failures:**
```bash
# Check Spark application logs
docker exec spark-master spark-submit \
  --deploy-mode client --driver-java-options "-Dlog4j.debug=true" \
  /opt/spark-jobs/failing_job.py

# Monitor Spark resources
curl http://localhost:8080/api/v1/applications

# Clear Spark checkpoints
docker exec spark-master rm -rf /tmp/spark-checkpoints/*

# Restart Spark cluster
docker-compose restart spark-master spark-worker1 spark-worker2
```

**Dashboard Not Loading:**
```bash
# Check Streamlit logs
docker logs dashboard --tail 50

# Verify port availability
netstat -an | grep 8501

# Restart dashboard with debug mode
docker-compose up dashboard --build

# Test dashboard health endpoint
curl http://localhost:8501/health
```

### Health Checks and Monitoring

```bash
# Complete cluster health check
./scripts/health_check.sh --comprehensive

# Individual service health
curl http://localhost:9870/jmx              # NameNode JMX
curl http://localhost:8501/metrics          # Dashboard metrics
curl http://localhost:8080/api/v1/status    # Spark status

# Resource usage monitoring
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

# Disk usage analysis
docker exec namenode hdfs dfs -du -h /data
docker system df -v
```

### Log Analysis

```bash
# Centralized log collection
./scripts/collect_logs.sh --output /tmp/cluster_logs/

# Search across all logs
./scripts/search_logs.sh --pattern "ERROR" --since "2023-12-15"

# Real-time log monitoring
docker-compose logs -f --tail 10 namenode datanode1 spark-master

# Export logs for analysis
./scripts/export_logs.sh --format json --timerange last_hour
```

## ğŸ“Š Performance Tuning

### Resource Allocation

Optimize Docker resource allocation in `docker-compose.yml`:

```yaml
spark-master:
  environment:
    - SPARK_WORKER_MEMORY=4G
    - SPARK_WORKER_CORES=4
    - SPARK_DRIVER_MEMORY=2G
  deploy:
    resources:
      limits:
        memory: 6G
        cpus: '4.0'
      reservations:
        memory: 4G
        cpus: '2.0'
```

### HDFS Configuration

Optimize HDFS performance in `hadoop-namenode/hdfs-site.xml`:

```xml
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
<property>
    <name>dfs.block.size</name>
    <value>268435456</value> <!-- 256MB blocks -->
</property>
<property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>8192</value>
</property>
```

### Kafka Performance Tuning

```bash
# Optimize Kafka for throughput
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type brokers --entity-name 1 \
  --alter --add-config num.network.threads=8,num.io.threads=16

# Batch processing optimization
docker exec kafka kafka-configs --bootstrap-server localhost:9092 \
  --entity-type topics --entity-name scraped-data \
  --alter --add-config batch.size=32768,linger.ms=10
```

## ğŸ”— Integration Points

This Hadoop cluster integrates with:

- **AI API Repository**: RESTful ML inference and batch processing
- **External data sources**: Real-time web scraping with rate limiting
- **Analytics tools**: Jupyter notebooks, Tableau, Power BI connectors
- **Monitoring systems**: Prometheus, Grafana, ELK stack integration
- **Cloud platforms**: AWS S3, Azure Blob, Google Cloud Storage
- **CI/CD pipelines**: Jenkins, GitLab CI, GitHub Actions
- **Message queues**: RabbitMQ, Apache Pulsar compatibility

### API Endpoints

```bash
# Cluster status API
GET http://localhost:9870/webhdfs/v1/?op=LISTSTATUS

# Spark job submission API  
POST http://localhost:8080/api/v1/submissions/create

# Dashboard API
GET http://localhost:8501/api/metrics
GET http://localhost:8501/api/data/recent

# Health check endpoints
GET http://localhost:9870/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState
```

## ğŸ“š Documentation

### Official Documentation
- [Hadoop Official Documentation](https://hadoop.apache.org/docs/)
- [Spark Programming Guide](https://spark.apache.org/docs/latest/)
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Hive User Guide](https://cwiki.apache.org/confluence/display/Hive/Home)

### Project-Specific Guides
- [Deployment Guide](./docs/deployment.md)
- [Troubleshooting Guide](./docs/troubleshooting.md)
- [API Reference](./docs/api_reference.md)
- [Performance Tuning](./docs/performance.md)

## ğŸ† Project Structure

```
adam_hadoop/
â”œâ”€â”€ ansible/                 # Automated deployment and configuration
â”‚   â”œâ”€â”€ playbooks/          # Ansible playbooks for cluster setup
â”‚   â”œâ”€â”€ inventory/          # Environment configurations
â”‚   â””â”€â”€ roles/              # Reusable Ansible roles
â”œâ”€â”€ dashboard/               # Streamlit monitoring dashboard  
â”‚   â”œâ”€â”€ pages/              # Multi-page dashboard components
â”‚   â”œâ”€â”€ components/         # Reusable UI components
â”‚   â”œâ”€â”€ utils/              # Dashboard utilities
â”‚   â””â”€â”€ app.py              # Main dashboard application
â”œâ”€â”€ data-loader/            # External dataset loading
â”‚   â”œâ”€â”€ loaders/            # Different data source loaders
â”‚   â”œâ”€â”€ transformers/       # Data transformation scripts
â”‚   â””â”€â”€ validators/         # Data quality validation
â”œâ”€â”€ docs/                   # Comprehensive documentation
â”‚   â”œâ”€â”€ api/                # API documentation
â”‚   â”œâ”€â”€ deployment/         # Deployment guides
â”‚   â””â”€â”€ troubleshooting/    # Issue resolution guides
â”œâ”€â”€ hadoop-namenode/        # NameNode configuration
â”‚   â”œâ”€â”€ config/             # Hadoop configuration files
â”‚   â””â”€â”€ scripts/            # NameNode-specific scripts
â”œâ”€â”€ hadoop-datanode/        # DataNode configuration
â”‚   â”œâ”€â”€ config/             # DataNode configurations
â”‚   â””â”€â”€ scripts/            # DataNode-specific scripts
â”œâ”€â”€ hive-config/            # Hive warehouse setup
â”‚   â”œâ”€â”€ schemas/            # Database schemas
â”‚   â”œâ”€â”€ tables/             # Table definitions
â”‚   â””â”€â”€ queries/            # Sample queries
â”œâ”€â”€ monitoring/             # Monitoring and alerting
â”‚   â”œâ”€â”€ prometheus/         # Prometheus configuration
â”‚   â”œâ”€â”€ grafana/           # Grafana dashboards
â”‚   â””â”€â”€ alerts/            # Alert configurations
â”œâ”€â”€ scraper/                # Real-time web scraping
â”‚   â”œâ”€â”€ sources/            # Different data source scrapers
â”‚   â”œâ”€â”€ processors/         # Data processing modules
â”‚   â””â”€â”€ config/             # Scraper configurations
â”œâ”€â”€ spark-jobs/             # Spark analytics jobs
â”‚   â”œâ”€â”€ batch/              # Batch processing jobs
â”‚   â”œâ”€â”€ streaming/          # Real-time streaming jobs
â”‚   â”œâ”€â”€ ml/                 # Machine learning pipelines
â”‚   â””â”€â”€ utils/              # Common utilities
â”œâ”€â”€ scripts/                # Deployment and management scripts
â”‚   â”œâ”€â”€ deploy.sh           # Main deployment script
â”‚   â”œâ”€â”€ backup.sh           # Backup utilities
â”‚   â”œâ”€â”€ restore.sh          # Restore utilities
â”‚   â””â”€â”€ health_check.sh     # Health monitoring
â”œâ”€â”€ tests/                  # Test suites
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â”œâ”€â”€ integration/        # Integration tests
â”‚   â””â”€â”€ performance/        # Performance tests
â”œâ”€â”€ docker-compose.yml      # Complete service orchestration
â”œâ”€â”€ docker-compose.dev.yml  # Development environment
â”œâ”€â”€ docker-compose.prod.yml # Production environment
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ Makefile               # Build and management commands
â””â”€â”€ README.md              # This comprehensive guide
```

## ğŸ¯ Use Cases

This infrastructure supports diverse applications:

### Academic and Research
- **Big data research projects** with distributed computing capabilities
- **Machine learning experiments** with large-scale datasets
- **Data mining studies** on social media and news content
- **Performance benchmarking** of big data technologies
- **Educational demonstrations** of Hadoop ecosystem components

### Business Applications
- **Real-time analytics** for streaming web content and social media
- **Content aggregation** for news and media companies
- **Market sentiment analysis** from social media and news sources
- **Data warehousing** for business intelligence and reporting
- **ETL pipelines** for data integration and transformation

### Technical Applications  
- **Proof of concept** for big data architectures
- **Development platform** for Spark and Hadoop applications
- **Testing environment** for data processing algorithms
- **Integration platform** for AI and ML workflows
- **Monitoring solution** for distributed systems

### Advanced Use Cases
- **Real-time recommendation systems** based on user behavior
- **Fraud detection** using streaming analytics
- **IoT data processing** for sensor data analysis
- **Log analysis** for security and compliance monitoring
- **A/B testing platform** for data-driven decision making

## ğŸ¤ Contributing

We welcome contributions to improve this Hadoop infrastructure:

### Getting Started
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Set up development environment (`./scripts/setup_dev.sh`)
4. Make your changes with comprehensive tests
5. Commit changes (`git commit -m 'Add amazing feature'`)
6. Push to branch (`git push origin feature/amazing-feature`)
7. Open Pull Request with detailed description

### Contribution Guidelines
- Follow existing code style and conventions
- Add comprehensive tests for new features
- Update documentation for any changes
- Ensure all tests pass before submitting PR
- Include performance impact analysis for major changes

### Areas for Contribution
- Additional data source scrapers
- New Spark analytics jobs
- Dashboard improvements and new visualizations
- Performance optimizations
- Documentation enhancements
- Test coverage improvements
- Security enhancements
- Cloud deployment templates

## ğŸ“„ License

This project is part of an academic assignment and is intended for educational purposes. 

**License Terms:**
- Educational use permitted
- Commercial use requires permission
- Attribution required for academic papers
- No warranty provided
- Subject to university academic integrity policies

## ğŸš€ What's Next?

### Planned Enhancements
- **Multi-machine cluster deployment** with Kubernetes orchestration
- **Elasticsearch integration** for advanced search and analytics capabilities
- **Cloud storage integration** (AWS S3, Azure Blob Storage, Google Cloud Storage)
- **Advanced ML pipelines** with MLflow experiment tracking
- **Real-time alerting system** with Slack/Teams integration
- **Security enhancements** with Kerberos authentication
- **Data governance** with Apache Atlas integration
- **Stream processing** with Apache Flink integration

### Performance Improvements
- **GPU acceleration** for AI workloads
- **Columnar storage** with Apache Parquet optimization
- **Caching layer** with Redis for frequently accessed data
- **Load balancing** for high-availability deployments
- **Auto-scaling** based on workload demands

### Monitoring and Observability
- **Distributed tracing** with Jaeger
- **Advanced metrics** with Prometheus and Grafana
- **Log aggregation** with ELK stack
- **Anomaly detection** in system metrics
- **Capacity planning** tools and forecasting

---

**ğŸ“ Ready to explore big data? Follow the prerequisites above, then deploy your cluster with `./scripts/deploy.sh` and start your data journey! ğŸš€**

**ğŸ“§ For questions or issues, please create a GitHub issue with detailed logs and system information.**