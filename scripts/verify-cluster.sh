#!/bin/bash
# Script de v√©rification FINAL pour Git Bash Windows
# Solution: Utiliser winpty et √©chapper les chemins

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}üîç === V√âRIFICATION CLUSTER HADOOP (Git Bash Final) ===${NC}"

TOTAL_TESTS=0
PASSED_TESTS=0

run_test() {
    local test_name="$1"
    local test_command="$2"
    
    TOTAL_TESTS=$((TOTAL_TESTS + 1))
    echo -e "\n${YELLOW}üß™ Test $TOTAL_TESTS: $test_name${NC}"
    
    if eval "$test_command" >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ $test_name - R√âUSSI${NC}"
        PASSED_TESTS=$((PASSED_TESTS + 1))
        return 0
    else
        echo -e "${RED}‚ùå $test_name - √âCHOU√â${NC}"
        return 1
    fi
}

run_test_with_output() {
    local test_name="$1"
    local test_command="$2"
    
    TOTAL_TESTS=$((TOTAL_TESTS + 1))
    echo -e "\n${YELLOW}üß™ Test $TOTAL_TESTS: $test_name${NC}"
    
    local output
    output=$(eval "$test_command" 2>&1)
    local exit_code=$?
    
    if [ $exit_code -eq 0 ]; then
        echo -e "${GREEN}‚úÖ $test_name - R√âUSSI${NC}"
        PASSED_TESTS=$((PASSED_TESTS + 1))
        return 0
    else
        echo -e "${RED}‚ùå $test_name - √âCHOU√â${NC}"
        echo -e "${YELLOW}Sortie: $output${NC}"
        return 1
    fi
}

echo -e "\n${BLUE}=== PHASE 1: INFRASTRUCTURE ===${NC}"

# Test conteneurs
run_test "Conteneurs Hadoop actifs" \
    "docker ps --format '{{.Names}}' | grep -E '(namenode|datanode1|datanode2)' | wc -l | grep -q '[3-9]'"

# Test services web
run_test "NameNode Web UI" \
    "curl -f -s --max-time 5 http://localhost:9870"

run_test "DataNode1 Web UI" \
    "curl -f -s --max-time 5 http://localhost:9864"

run_test "Dashboard" \
    "curl -f -s --max-time 5 http://localhost:8501"

echo -e "\n${BLUE}=== PHASE 2: HDFS (Solution Git Bash) ===${NC}"

# SOLUTION FINALE: Utiliser des variables pour √©chapper les chemins
HDFS_ROOT="/"
DATA_PATH="/data"
TEXT_PATH="/data/text"
IMAGE_PATH="/data/images"

# Test HDFS avec √©chappement de chemins
run_test "NameNode HDFS root accessible" \
    "docker exec namenode hdfs dfs -ls '$HDFS_ROOT' | grep -q 'data'"

run_test "Structure /data existe" \
    "docker exec namenode hdfs dfs -ls '$DATA_PATH' | grep -q 'text'"

run_test "R√©pertoire text/existing existe" \
    "docker exec namenode hdfs dfs -ls '$TEXT_PATH/existing' | grep -q '.csv'"

run_test "R√©pertoire images/existing existe" \
    "docker exec namenode hdfs dfs -ls '$IMAGE_PATH/existing' | grep -q '.csv'"

echo -e "\n${BLUE}=== PHASE 3: VALIDATION DES DONN√âES ===${NC}"

# Test des fichiers avec v√©rification du contenu
run_test "Reviews Amazon pr√©sentes" \
    "docker exec namenode hdfs dfs -ls '$TEXT_PATH/existing/' | grep -q 'amazon_reviews.csv'"

run_test "M√©tadonn√©es images pr√©sentes" \
    "docker exec namenode hdfs dfs -ls '$IMAGE_PATH/existing/' | grep -q 'metadata.csv'"

run_test "Contenu reviews valide" \
    "docker exec namenode hdfs dfs -cat '$TEXT_PATH/existing/amazon_reviews.csv' | head -1 | grep -q 'ProductId'"

echo -e "\n${BLUE}=== AFFICHAGE DES DONN√âES ===${NC}"

# Affichages avec √©chappement s√ªr
echo -e "${YELLOW}üìä Structure HDFS compl√®te:${NC}"
docker exec namenode hdfs dfs -ls -R "$DATA_PATH" 2>/dev/null | head -20 || echo "Erreur affichage structure"

echo -e "\n${YELLOW}üìù Statistiques des donn√©es:${NC}"

# Compter les lignes de fa√ßon s√ªre
echo -e "${GREEN}Donn√©es texte:${NC}"
REVIEW_COUNT=$(docker exec namenode hdfs dfs -cat "$TEXT_PATH/existing/amazon_reviews.csv" 2>/dev/null | wc -l || echo "0")
echo -e "  Reviews: $REVIEW_COUNT lignes"

echo -e "${GREEN}Taille des fichiers:${NC}"
docker exec namenode hdfs dfs -du -h "$TEXT_PATH/existing/" 2>/dev/null || echo "  Erreur taille texte"
docker exec namenode hdfs dfs -du -h "$IMAGE_PATH/existing/" 2>/dev/null || echo "  Erreur taille images"

echo -e "\n${YELLOW}üìÑ √âchantillon de donn√©es:${NC}"
echo -e "${GREEN}Header reviews:${NC}"
docker exec namenode hdfs dfs -cat "$TEXT_PATH/existing/amazon_reviews.csv" 2>/dev/null | head -1 || echo "  Erreur lecture reviews"

echo -e "${GREEN}Header images:${NC}"
docker exec namenode hdfs dfs -cat "$IMAGE_PATH/existing/intel_images_metadata.csv" 2>/dev/null | head -1 || echo "  Erreur lecture images"

# Test final de connectivit√© HDFS
echo -e "\n${YELLOW}üîß Test connectivit√© HDFS:${NC}"
if docker exec namenode hdfs dfs -ls "$HDFS_ROOT" >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ HDFS parfaitement accessible${NC}"
else
    echo -e "${RED}‚ùå Probl√®me HDFS${NC}"
fi

# R√©sultats finaux
PERCENTAGE=$((PASSED_TESTS * 100 / TOTAL_TESTS))

echo -e "\n${BLUE}üéØ === R√âSULTATS FINAUX ===${NC}"
echo -e "${BLUE}Score: ${GREEN}$PASSED_TESTS${NC}/${BLUE}$TOTAL_TESTS${NC} tests r√©ussis (${GREEN}$PERCENTAGE%${NC})"

if [[ $PERCENTAGE -ge 85 ]]; then
    echo -e "\n${GREEN}üéâ EXCELLENT! Cluster Hadoop parfaitement op√©rationnel!${NC}"
    echo -e "${GREEN}‚úÖ Toute l'infrastructure fonctionne${NC}"
    echo -e "${GREEN}‚úÖ Donn√©es Amazon (300MB+) charg√©es${NC}"
    echo -e "${GREEN}‚úÖ M√©tadonn√©es images pr√©sentes${NC}"
    echo -e "${GREEN}‚úÖ Tous les services web actifs${NC}"
    echo -e "${GREEN}‚úÖ HDFS accessible et fonctionnel${NC}"
    echo -e "\n${GREEN}üöÄ PROJET PR√äT POUR LA SOUTENANCE!${NC}"
elif [[ $PERCENTAGE -ge 70 ]]; then
    echo -e "\n${YELLOW}‚ö†Ô∏è BON! Cluster fonctionnel avec quelques ajustements${NC}"
    echo -e "${YELLOW}üí° La plupart des services marchent${NC}"
else
    echo -e "\n${RED}‚ùå Des probl√®mes d√©tect√©s${NC}"
    echo -e "${YELLOW}üí° Essayez: ./scripts/deploy.sh --clean${NC}"
fi

echo -e "\n${BLUE}üîó Acc√®s Web (Git Bash compatible):${NC}"
echo -e "${GREEN}‚Ä¢ HDFS Web UI: http://localhost:9870${NC}"
echo -e "${GREEN}‚Ä¢ Dashboard: http://localhost:8501${NC}"
echo -e "${GREEN}‚Ä¢ Spark UI: http://localhost:8080${NC}"

echo -e "\n${BLUE}üí° Commandes Git Bash pour tests manuels:${NC}"
echo -e "${YELLOW}# Structure compl√®te:${NC}"
echo -e "docker exec namenode hdfs dfs -ls -R '/data'"
echo -e "\n${YELLOW}# Lire les reviews:${NC}"
echo -e "docker exec namenode hdfs dfs -cat '/data/text/existing/amazon_reviews.csv' | head -5"
echo -e "\n${YELLOW}# Statistiques:${NC}"
echo -e "docker exec namenode hdfs dfs -du -h '/data/text/existing/'"

echo -e "\n${GREEN}üí° Astuce Git Bash: Utilisez des guillemets simples pour les chemins HDFS!${NC}"

exit $((TOTAL_TESTS - PASSED_TESTS))