#!/bin/bash
# Nettoyage complet et red√©marrage de Hadoop

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${RED}üßπ === NETTOYAGE COMPLET HADOOP ===${NC}"
echo -e "${YELLOW}Ce script va tout supprimer et red√©marrer proprement${NC}"

# 1. Arr√™ter tous les conteneurs
echo -e "\n${YELLOW}‚èπÔ∏è Arr√™t de tous les conteneurs...${NC}"
docker-compose down --remove-orphans -v || true
docker-compose down || true

# 2. Supprimer tous les conteneurs Hadoop (m√™me orphelins)
echo -e "\n${YELLOW}üóëÔ∏è Suppression des conteneurs orphelins...${NC}"
docker ps -a --format "{{.Names}}" | grep -E "(namenode|datanode|dashboard|spark|kafka|hive)" | xargs -r docker rm -f || true

# 3. Supprimer tous les volumes Hadoop
echo -e "\n${YELLOW}üíæ Suppression des volumes persistants...${NC}"
docker volume ls -q | grep -E "(hadoop|namenode|datanode)" | xargs -r docker volume rm -f || true

# 4. Nettoyer les volumes sp√©cifiques du compose
echo -e "\n${YELLOW}üßπ Nettoyage volumes Docker Compose...${NC}"
docker volume rm -f $(docker volume ls -q | grep "adam_hadoop" || true) 2>/dev/null || true

# 5. Nettoyage r√©seau
echo -e "\n${YELLOW}üåê Nettoyage r√©seaux...${NC}"
docker network rm $(docker network ls -q | grep "hadoop") 2>/dev/null || true

# 6. Nettoyage g√©n√©ral Docker
echo -e "\n${YELLOW}üßΩ Nettoyage g√©n√©ral Docker...${NC}"
docker system prune -f
docker volume prune -f

echo -e "\n${GREEN}‚úÖ Nettoyage complet termin√©!${NC}"

# 7. V√©rification qu'il ne reste rien
echo -e "\n${BLUE}üîç V√©rification finale...${NC}"
remaining_containers=$(docker ps -a --format "{{.Names}}" | grep -E "(namenode|datanode|dashboard)" || true)
if [ -n "$remaining_containers" ]; then
    echo -e "${RED}‚ùå Des conteneurs restent:${NC}"
    echo "$remaining_containers"
else
    echo -e "${GREEN}‚úÖ Aucun conteneur Hadoop restant${NC}"
fi

remaining_volumes=$(docker volume ls -q | grep -E "(hadoop|namenode|datanode)" || true)
if [ -n "$remaining_volumes" ]; then
    echo -e "${RED}‚ùå Des volumes restent:${NC}"
    echo "$remaining_volumes"
else
    echo -e "${GREEN}‚úÖ Aucun volume Hadoop restant${NC}"
fi

echo -e "\n${BLUE}üöÄ === D√âMARRAGE PROPRE HADOOP ===${NC}"

# 8. Reconstruire et d√©marrer dans le bon ordre
echo -e "\n${YELLOW}üì¶ Construction des images...${NC}"
docker-compose build --no-cache

echo -e "\n${YELLOW}üñ•Ô∏è D√©marrage du NameNode d'abord...${NC}"
docker-compose up -d namenode

echo -e "\n${YELLOW}‚è≥ Attente NameNode (45s)...${NC}"
sleep 45

# V√©rifier que NameNode fonctionne
echo -e "\n${BLUE}üîç V√©rification NameNode...${NC}"
if curl -f -s http://localhost:9870 >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ NameNode op√©rationnel${NC}"
else
    echo -e "${RED}‚ùå NameNode pas encore pr√™t, attente 30s de plus...${NC}"
    sleep 30
fi

echo -e "\n${YELLOW}üìä D√©marrage des DataNodes...${NC}"
docker-compose up -d datanode1 datanode2

echo -e "\n${YELLOW}‚è≥ Attente connexion DataNodes (60s)...${NC}"
sleep 60

# V√©rifier les DataNodes
echo -e "\n${BLUE}üîç V√©rification DataNodes...${NC}"
datanode_count=0

if curl -f -s http://localhost:9864 >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ DataNode1 op√©rationnel${NC}"
    ((datanode_count++))
else
    echo -e "${RED}‚ùå DataNode1 non accessible${NC}"
fi

if curl -f -s http://localhost:9865 >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ DataNode2 op√©rationnel${NC}"
    ((datanode_count++))
else
    echo -e "${RED}‚ùå DataNode2 non accessible${NC}"
fi

echo -e "\n${BLUE}üìä DataNodes connect√©s: $datanode_count/2${NC}"

# V√©rifier la connexion HDFS
echo -e "\n${BLUE}üîç Test connexion HDFS...${NC}"
if docker exec namenode hdfs dfs -ls / >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ HDFS accessible${NC}"
else
    echo -e "${RED}‚ùå HDFS non accessible, attente 30s...${NC}"
    sleep 30
fi

echo -e "\n${YELLOW}üåê D√©marrage services compl√©mentaires...${NC}"
docker-compose up -d

echo -e "\n${YELLOW}‚è≥ Attente stabilisation cluster (30s)...${NC}"
sleep 30

# Test final HDFS
echo -e "\n${BLUE}üß™ Test final HDFS...${NC}"
if docker exec namenode hdfs dfs -ls / >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ HDFS compl√®tement op√©rationnel${NC}"
    
    # Test rapport admin
    echo -e "\n${BLUE}üìä Rapport cluster:${NC}"
    docker exec namenode hdfs dfsadmin -report | head -10
else
    echo -e "${RED}‚ùå HDFS encore probl√©matique${NC}"
fi

echo -e "\n${GREEN}üéâ === RED√âMARRAGE COMPLET TERMIN√â ===${NC}"

# Instructions pour charger les donn√©es
echo -e "\n${BLUE}üìã === PROCHAINES √âTAPES ===${NC}"
echo -e "${YELLOW}1. V√©rifiez le status:${NC}"
echo -e "   ./scripts/deploy.sh --status"
echo -e "\n${YELLOW}2. Si tout est OK, chargez les donn√©es:${NC}"
echo -e "   docker-compose run --rm data-loader"
echo -e "\n${YELLOW}3. V√©rifiez les donn√©es:${NC}"
echo -e "   docker exec namenode hdfs dfs -ls /data"

echo -e "\n${GREEN}üîó Acc√®s:${NC}"
echo -e "‚Ä¢ NameNode: http://localhost:9870"
echo -e "‚Ä¢ DataNode1: http://localhost:9864"
echo -e "‚Ä¢ DataNode2: http://localhost:9865"
echo -e "‚Ä¢ Dashboard: http://localhost:8501"