from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Cr√©er la session Spark
spark = SparkSession.builder \
    .appName("NewsStreamProcessor") \
    .config("spark.sql.adaptive.enabled", "false") \
    .getOrCreate()

print("‚úÖ Spark session cr√©√©e avec succ√®s!")

# Test simple : cr√©er des donn√©es de test
data = [
    ("1", "Breaking: New AI Technology Revolutionizes Healthcare", "https://example.com/1", "2025-06-22T18:46:00", "hackernews"),
    ("2", "Python 3.12 Released with Amazing Features!", "https://example.com/2", "2025-06-22T18:47:00", "hackernews"),
    ("3", "Docker Containers vs Virtual Machines: Performance Comparison", "https://example.com/3", "2025-06-22T18:48:00", "hackernews")
]

schema = StructType([
    StructField("id", StringType()),
    StructField("title", StringType()),
    StructField("url", StringType()),
    StructField("timestamp", StringType()),
    StructField("source", StringType())
])

# Cr√©er un DataFrame de test
df = spark.createDataFrame(data, schema)

print("üìÑ Donn√©es de test cr√©√©es:")
df.show(truncate=False)

# Pr√©traitement simple
processed_df = df \
    .withColumn("title_length", length(col("title"))) \
    .withColumn("word_count", size(split(col("title"), " "))) \
    .withColumn("title_cleaned", 
        regexp_replace(
            regexp_replace(col("title"), "http\\S+", ""),  # Enlever URLs
            "[^a-zA-Z0-9\\s]", " "  # Enlever caract√®res sp√©ciaux
        )
    ) \
    .withColumn("title_cleaned", regexp_replace(col("title_cleaned"), "\\s+", " ")) \
    .withColumn("processed_time", current_timestamp())

print("üîß Donn√©es apr√®s pr√©traitement:")
processed_df.show(truncate=False)

# Test sauvegarde HDFS
try:
    processed_df.write \
        .mode("overwrite") \
        .parquet("hdfs://namenode:9000/data/test/news_sample")
    print("‚úÖ Sauvegarde HDFS r√©ussie!")
except Exception as e:
    print(f"‚ùå Erreur HDFS: {e}")

# Arr√™ter Spark
spark.stop()
print("üõë Test termin√©")